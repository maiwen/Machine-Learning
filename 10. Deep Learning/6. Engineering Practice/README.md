# 实践方法论

要成功地使用深度学习技术，仅仅知道存在哪些算法和解释它们为何有效的原理是不够的。
一个优秀的机器学习实践者还需要知道如何**针对具体应用挑选一个合适的算法**以及如何**监控**，并**根据实验反馈改进机器学习系统**。
在机器学习系统的日常开发中，
实践者需要决定是否收集更多的数据、增加或减少模型容量、添加或删除正则化项、改进模型的优化、改进模型的近似推断或调试模型的软件实现。
尝试这些操作都需要大量时间，因此确定正确的做法，而不盲目猜测尤为重要。

在实践中，正确使用一个普通算法通常比草率地使用一个不清楚的算法效果更好。
正确应用一个算法需要掌握一些相当简单的方法论。
我们建议参考以下几个实践设计流程：

* 确定目标——使用什么样的误差度量，并为此误差度量指定目标值。这些目标和误差度量取决于该应用旨在解决的问题。
* 尽快建立一个端到端的工作流程，包括估计合适的性能度量。
* 搭建系统，并确定性能瓶颈。检查哪个部分的性能差于预期，以及是否是因为过拟合、欠拟合，或者数据或软件缺陷造成的。
* 根据具体观察反复地进行增量式的改动，如收集新数据、调整超参数或改进算法。

## 1. 性能度量
确定目标，即使用什么误差度量，是必要的第一步，因为误差度量将指导接下来的所有工作。

### 训练数据的数量
训练数据的数量会因为各种原因受到限制。
当目标是打造现实世界中最好的产品或服务时，我们通常需要收集更多的数据，但必须确定进一步减少误差的价值，
并**与收集更多数据的成本做权衡**。数据收集会耗费时间、金钱，或带来人体痛苦（例如，收集人体医疗测试数据）。
科研中，目标通常是在某个确定基准下探讨哪个算法更好，一般会固定训练集，不允许收集更多的数据。

### 度量的选择

#### 精确度 （precision）和召回率 （recall）
我们通常会度量一个系统的准确率，或等价地，错误率。然而，许多应用需要更高级的度量。

有时，**一种错误可能会比另一种错误更严重**。
例如，垃圾邮件检测系统会有两种错误：将正常邮件错误地归为垃圾邮件，将垃圾邮件错误地归为正常邮件。
我们希望度量某种形式的总代价，其中拦截正常邮件比允许垃圾邮件通过的代价更高，而不是度量垃圾邮件分类的错误率。

有时，我们需要训练检测某些**罕见事件的二元分类器**。

例如，我们可能会为一种罕见疾病设计医疗测试。假设每一百万人中只有一人患病。
我们只需要让分类器一直报告没有患者，就能轻易地在检测任务上实现99.9999％的正确率。显然，**正确率很难描述这种系统的性能**。

解决这个问题的方法是度量精度 （precision）和召回率 （recall）。
精度是模型报告的检测正确的比率，而召回率则是真实事件被检测到的比率。
检测器永远报告没有患者，会得到一个完美的精度，但召回率为零。
而报告每个人都是患者的检测器会得到一个完美的召回率，但是精度会等于人群中患有该病的比例（在我们的例子中是0.0001％，即每一百万人只有一人患病）。

当使用精度和召回率时，我们通常会画**PR曲线** （PR curve），y轴表示精度，x轴表示召回率。
例如，我们将前馈网络设计为检测一种疾病，估计一个医疗结果由特征 x 表示的人患病的概率为 。
每当这个得分超过某个阈值时，我们报告检测结果。通过调整阈值，我们能**权衡精度和召回率**。
在很多情况下，我们希望用一个数而不是曲线来概括分类器的性能。要做到这一点，我们可以将精度p和召回率r转换为F分数 （F-score）
F = 2pr / (p+r)

另一种方法是报告PR曲线下方的总面积。

#### 覆盖
如果机器学习算法能够估计所作判断的置信度，这将会非常有用，特别是在错误判断会导致严重危害，而人工操作员能够偶尔接管的情况下。

覆盖是机器学习系统能够产生响应的样本所占的比率。我们权衡覆盖和精度。一个系统可以通过拒绝处理任意样本的方式来达到100％的精度，但是覆盖降到了0％。

还有许多其他的性能度量。最重要的是首先要确定改进哪个性能度量，然后专心提高性能度量。如果没有明确的目标，那么我们很难判断机器学习系统上的改动是否有所改进。

## 2. 基准模型
确定性能度量和目标后，任何实际应用的下一步是尽快建立一个合理的端到端的系统。
下面提供了关于不同情况下使用哪种算法作为第一基准方法的推荐。值得注意的是，深度学习研究进展迅速，很快可能会有更好的默认算法。

根据问题的复杂性，项目开始时可能无须使用深度学习。
如果只需正确地选择几个线性权重就可能解决问题，那么项目可以开始于一个简单的统计模型，如逻辑回归。

如果问题属于“AI-完全”类的，如对象识别、语音识别、机器翻译等，那么项目开始于一个合适的深度学习模型，效果会比较好。

**首先，根据数据的结构选择一类合适的模型**。如果项目是以固定大小的向量作为输入的监督学习，那么可以使用全连接的前馈网络。
如果输入已知的拓扑结构（例如，输入的是图像），那么可以使用卷积网络。
在这些情况下，刚开始可以使用某些分段线性单元（ReLU或者其扩展，如Leaky ReLU、PReLU和maxout）。
如果输入或输出是一个序列，可以使用门控循环网络（LSTM或GRU）。

具有衰减学习率以及动量的SGD是优化算法一个合理的选择
（流行的衰减方法有，衰减到固定最低学习率的线性衰减、指数衰减，或每次发生验证错误停滞时将学习率降低2～10倍，这些衰减方法在不同问题上好坏不一）。
另一个非常合理的选择是Adam算法。
**批标准化**对优化性能有着显著的影响，特别是对卷积网络和具有sigmoid非线性函数的网络而言。
虽然在最初的基准中忽略批标准化是合理的，然而当优化似乎出现问题时，应该立刻使用批标准化。

除非训练集包含数千万以及更多的样本，否则项目应该在一开始就包含一些温和的正则化。
提前终止也被普遍采用。Dropout也是一个很容易实现，且兼容很多模型和训练算法的出色正则化项。
批标准化有时也能降低泛化误差，此时可以省略Dropout步骤，因为用于标准化变量的统计量估计本身就存在噪声。

如果我们的任务和另一个被广泛研究的任务相似，那么通过**复制先前研究中已知性能良好的模型和算法**，可能会得到很好的效果，
甚至可以从该任务中**复制一个训练好的模型**。
例如，通常会使用在ImageNet上训练好的卷积网络的特征来解决其他计算机视觉任务（Girshick et al. ，2015）。

## 3. 是否需要收集更多数据？
在建立第一个端到端的系统后，就可以度量算法性能并决定如何改进算法。
许多机器学习新手都忍不住尝试很多不同的算法来进行改进。然而，**收集更多的数据**往往比改进学习算法要有用得多。

怎样判断是否要收集更多的数据？

首先，确定训练集上的性能是否可接受。
如果模型在训练集上的性能就很差，学习算法都不能在训练集上学习出良好的模型，那么就没必要收集更多的数据，
可以尝试增加更多的网络层或每层增加更多的隐藏单元，以增加模型的规模。
此外，也可以尝试调整学习率等超参数的措施来改进学习算法。
如果更大的模型和仔细调试的优化算法效果不佳，那么问题可能源自**训练数据的质量**。
数据可能含太多噪声，或是可能不包含预测输出所需的正确输入。这意味着我们需要重新开始，收集更干净的数据或是收集特征更丰富的数据集。

如果训练集上的性能是可接受的，那么我们开始度量测试集上的性能。如果测试集上的性能也是可以接受的，那么就顺利完成了。

如果测试集上的性能比训练集的要差得多，那么收集更多的数据是最有效的解决方案之一。
这时主要的考虑是收集更多数据的代价和可行性，其他方法降低测试误差的代价和可行性，以及增加数据数量能否显著提升测试集性能。
在拥有百万甚至上亿用户的大型网络公司，收集大型数据集是可行的，并且这样做的成本可能比其他方法要少很多，所以答案几乎总是收集更多的训练数据。
例如，收集大型标注数据集是解决对象识别问题的主要因素之一。在其他情况下，如医疗应用，收集更多的数据可能代价很高或者不可行。
一个可以替代的简单方法是降低模型大小或是改进正则化（调整超参数，如权重衰减系数，或是加入正则化策略，如Dropout）。
如果调整正则化超参数后，训练集性能和测试集性能之间的差距还是不可接受，那么收集更多的数据是可取的。

在决定是否收集更多的数据时，也需要**确定收集多少数据**。
绘制曲线显示训练集规模和泛化误差之间的关系是很有帮助的。根据走势延伸曲线，可以预测还需要多少训练数据来达到一定的性能。
通常，加入总数目一小部分的样本不会对泛化误差产生显著的影响。因此，建议**在对数尺度上考虑训练集的大小**，例如在后续的实验中倍增样本数目。

如果收集更多的数据是不可行的，那么改进泛化误差的唯一方法是改进学习算法本身。这属于研究领域，并非对应用实践者的建议。

## 3. 选择超参数

大部分深度学习算法都有许多超参数来控制不同方面的算法表现。
有些超参数会影响**算法运行的时间和存储成本**，有些超参数会影响学习到的**模型质量**以及在新输入上推断正确结果的能力。

有两种选择超参数的基本方法：手动选择和自动选择。手动选择超参数需要了解超参数做了些什么，以及机器学习模型如何才能取得良好的泛化。
自动选择超参数算法大大减少了了解这些想法的需要，但它们往往需要更高的计算成本。

### 手动调整超参数
手动设置超参数，我们必须了解超参数、训练误差、泛化误差和计算资源（内存和运行时间）之间的关系。这需要切实了解一个学习算法有效容量的基础概念，如第5章所描述的。

手动搜索超参数的目标通常是最小化受限于运行时间和内存预算的泛化误差。

学习率可能是最重要的超参数。如果你只有时间调整一个超参数，那就调整学习率。
相比其他超参数，它以一种更复杂的方式控制模型的有效容量——当学习率适合优化问题时，模型的有效容量最高，此时学习率是正确的，既不是特别大也不是特别小。
学习率关于训练误差具有U形曲线，如图11.1所示。
当学习率过大时，梯度下降可能会不经意地增加而非减少训练误差。
在理想化的二次情况下，如果学习率是最佳值的两倍大时，则会发生这种情况（LeCun et al. ，1998b）。
当学习率太小，训练不仅慢，还有可能永久停留在一个很高的训练误差上。关于这种效应，我们知之甚少（不会发生于一个凸损失函数中）。

调整学习率外的其他参数时，需要同时监测训练误差和测试误差，以判断模型是否过拟合或欠拟合，然后适当调整其容量。
通常，最佳性能来自正则化得很好的大规模模型，比如使用Dropout的神经网络。
大部分超参数可以通过推理其是否增加或减少模型容量来设置。

### 自动超参数优化算法
如果仔细想想使用者搜索学习算法合适超参数的方式，我们会意识到这其实是一种优化：我们在试图寻找超参数来优化目标函数，例如验证误差，有时还会有一些约束（如训练时间、内存或识别时间的预算）。因此，原则上有可能开发出封装学习算法的超参数优化 （hyperparameter optimization）算法，并选择其超参数，从而使用者不需要指定学习算法的超参数。令人遗憾的是，超参数优化算法往往有自己的超参数，如学习算法的每个超参数应该被探索的值的范围。
然而，这些次级超参数通常很容易选择，这就是说，相同的次级超参数能够在很多不同的问题上具有良好的性能。

#### 网格搜索
对于每个超参数，使用者选择一个较小的有限值集去探索。
然后，这些超参数笛卡儿乘积得到一组组超参数，网格搜索使用每组超参数训练模型。挑选验证集误差最小的超参数作为最好的超参数。

通常重复进行网格搜索时，效果会最好。例如，假设我们在集合{-1，0，1}上网格搜索超参数α。如果找到的最佳值是1，那么说明我们低估了最优值α所在的范围，应该改变搜索格点，例如在集合{1，2，3}中搜索。如果最佳值是0，那么我们不妨通过细化搜索范围以改进估计，在集合{-0.1，0，0.1}上进行网格搜索。

网格搜索带来的一个明显问题是，计算代价会随着超参数数量呈指数级增长。如果有m个超参数，每个最多取n个值，那么训练和估计所需的试验数将是O(n**m )。

#### 随机搜索
随机搜索过程如下。首先，我们为每个超参数定义一个边缘分布，例如，Bernoulli分布或范畴分布（分别对应着二元超参数或离散超参数），或者对数尺度上的均匀分布（对应着正实值超参数）。例如，
![](../img/sjss.png)

其中，u(a,b)表示区间(a,b)上均匀采样的样本。类似地，log_number_of_hidden_units 可以从u(log(50),log(2000))上采样。

与网格搜索不同，我们不需要离散化超参数的值。这允许我们在一个更大的集合上进行搜索，而不产生额外的计算代价。

#### 基于模型的超参数优化
超参数搜索问题可以转化为一个优化问题，决策变量是超参数，优化的代价是超参数训练出来的模型在验证集上的误差。在简化的设定下，可以计算验证集上可导误差函数关于超参数的梯度，然后我们遵循这个梯度更新（Bengio et al. ，1999；Bengio，2000；Maclaurin et al. ，2015）。令人遗憾的是，在大多数实际设定中，这个梯度是不可用的。这可能是因为其高额的计算代价和存储成本，也可能是因为验证集误差在超参数上本质上不可导，例如超参数是离散值的情况。

大部分超参数优化算法比随机搜索更复杂，并且具有一个共同的缺点，在它们能够从实验中提取任何信息之前，它们需要运行完整的训练实验。相比于人类实践者手动搜索，考虑实验早期可以收集的信息量，这种方法是相当低效的，因为手动搜索通常可以很早判断出某组超参数是否是完全病态的。

## 4. 调试策略

大部分神经网络的调试策略都是解决这两个难题中的一个或两个:
1. 在大多数情况下，我们不能提前知道算法的行为。事实上，使用机器学习的整个出发点是，它会发现一些我们自己无法发现的有用行为。
2. 大部分机器学习模型有多个自适应的部分, 如果一个部分失效了，其他部分仍然可以自适应，并获得大致可接受的性能。

我们可以设计一种足够简单的情况，能够提前得到正确结果，判断模型预测是否与之相符；我们也可以设计一个测试，独立检查神经网络实现的各个部分。

一些重要的调试如下所述:

* 可视化计算中模型的行为

当训练模型检测图像中的对象时，查看一些模型检测到部分重叠的图像。在训练语音生成模型时，试听一些生成的语音样本。这似乎是显而易见的，但在实际中很容易只注意量化性能度量，如准确率或对数似然。直接观察机器学习模型运行其任务，有助于确定其达到的量化性能数据是否看上去合理。

* 可视化最严重的错误

实际上模型的较小概率不太可能对应着正确的标签，因此它们在一定意义上还是有些用的。通过查看训练集中很难正确建模的样本，通常可以发现该数据预处理或者标记方式的问题。

* 根据训练和测试误差进行检测

我们往往很难确定底层软件是否正确实现。训练和测试误差能够提供一些线索。如果训练误差较低，但是测试误差较高，那么很有可能训练过程是在正常运行，但模型由于算法原因过拟合了。另一种可能是，测试误差没有被正确地度量，可能是由于训练后保存模型再重载去度量测试集时出现问题，或者是因为测试数据和训练数据预处理的方式不同。如果训练和测试误差都很高，那么很难确定是软件错误，还是由于算法原因模型欠拟合。这种情况需要进一步的测试，如下面所述。

* 拟合极小的数据集

当训练集上有很大的误差时，我们需要确定问题是真正的欠拟合，还是软件错误。通常，即使是小模型也可以保证很好地拟合一个足够小的数据集。

* 比较反向传播导数和数值导数

如果正在使用一个需要实现梯度计算的软件框架，那么常见的错误原因是没能正确地实现梯度表达.(使用框架应不会有此问题)

* 监控激活函数值和梯度的直方图

可视化神经网络在大量训练迭代后（也许是一个轮）收集到的激活函数值和梯度的统计量往往是有用的。
隐藏单元的预激活值可以告诉我们该单元是否饱和，或者它们饱和的频率如何。
在深度网络中，传播梯度的快速增长或快速消失，可能会阻碍优化过程。
比较参数梯度和参数的量级也是有帮助的，正如（Bottou，2015）所建议的，我们希望参数在一个小批量更新中变化的幅度是参数量值1％这样的级别，而不是50％或者0.001％（这会导致参数移动得太慢）。也有可能是某些参数以良好的步长移动，而另一些停滞。如果数据是稀疏的（比如自然语言），有些参数可能很少更新，检测它们变化时应该记住这一点。
